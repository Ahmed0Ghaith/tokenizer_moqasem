# Tokenizer Moqasem ๐



**ููุชุจุฉ ุฐููุฉ ููุนุงูุฌุฉ ุงููุตูุต ูุน ุฏุนู ูุชูุฏู ููุบุฉ ุงูุนุฑุจูุฉุ ูุทุงุจูุฉ ุบุงูุถุฉุ ูุชุญููู ูุตู ูุฏุนูู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู**

> โก **ููุงุญุธุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู:** ุชู ุฅูุดุงุก ูุฐู ุงูููุชุจุฉ ุจูุณุจุฉ 95% ุจูุณุงุนุฏุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุ ููุง ููุธูุฑ ููุฉ ุฃุฏูุงุช ุงูุชุทููุฑ ุงูุญุฏูุซุฉ ุงููุฏุนููุฉ ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู.

---

## โจ ุงููููุฒุงุช

### ๐ค ุชูุทูุน ูุตู ูุชูุฏู
- **ุงุณุชุฑุงุชูุฌูุงุช ูุชุนุฏุฏุฉ**: ูููุงุชุ ุฌููุ ุฃุญุฑูุ ูุณุงูุงุชุ n-gramุ ุฃููุงุท ูุฎุตุตุฉ
- **ุฏุนู ูุชุนุฏุฏ ุงููุบุงุช**: ุงูุนุฑุจูุฉุ ุงูุฅูุฌููุฒูุฉุ ูุงููุตูุต ุงููุฎุชูุทุฉ
- **ูุนุงูุฌุฉ ุฐููุฉ**: ุญุฐู ุงูุฃุญุฑู ุงูุตุบูุฑุฉุ ุนูุงูุงุช ุงูุชุฑูููุ ููููุงุช ุงูุชููู ุงููุงุจูุฉ ููุชุฎุตูุต

### ๐ฏ ุงููุทุงุจูุฉ ุงูุบุงูุถุฉ (Fuzzy Matching)
- **ูุณุงูุฉ Levenshtein**: ุญุณุงุจ ุงููุณุงูุฉ ุงูุชุญุฑูุฑูุฉ ุจูู ุงููุตูุต
- **ุชุดุงุจู ุงููููุงุช**: ุชุณุฌูู ุจุงููุณุจุฉ ุงููุฆููุฉ
- **ุชุญูู ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ**: ุฅูุฌุงุฏ ุงููุทุงุจูุงุช ุญุชู ูุน ูุฌูุฏ ุฃุฎุทุงุก
- **ุนุชุจุฉ ูุงุจูุฉ ููุชุฎุตูุต**: ุถุจุท ุญุณุงุณูุฉ ุงููุทุงุจูุฉ

### ๐ธ๐ฆ ุฏุนู ูุชูุฏู ููุบุฉ ุงูุนุฑุจูุฉ
- **ุชุทุจูุน ุงููุต**: ุฅุฒุงูุฉ ุงูุชุดููู ุชููุงุฆูุงู
- **ุชูููุนุงุช ุงูุฃูู**: ุชุทุจูุน ุฃุ ุฅุ ุข โ ุง
- **ุงูุชุงุก ุงููุฑุจูุทุฉ**: ูุนุงูุฌุฉ ุฉ โ ู
- **ุชุญููู ุงูุฃุฑูุงู**: ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ (ู-ูฉ) ุฅูู ุฅูุฌููุฒูุฉ (0-9)
- **ุงููุทุงุจูุฉ ุงูุตูุชูุฉ**: ุชุดุงุจู ุตูุชู ููุฃุญุฑู ุงูุนุฑุจูุฉ
- **ุงุณุชุฎุฑุงุฌ ุงูุฌุฐุฑ**: ุงุณุชุฎูุงุต ุฌุฐูุฑ ุงููููุงุช ุงูุนุฑุจูุฉ (Stemming)

### ๐ ุชุญููู ุงููุตูุต
- **ููุงููุณ ุงูุชุดุงุจู**: Jaccard ู Cosine similarity
- **ุชูุฑุงุฑ ุงููููุงุช**: ุชุญููู ุนุฏุฏ ูุชูุฑุงุฑ ุงููููุงุช
- **ูุดู ุงููุบุฉ**: ุชุญุฏูุฏ ุงููุบุฉ ุชููุงุฆูุงู
- **ุชุญููู ุงููุดุงุนุฑ**: ุชุณุฌูู ุงููุดุงุนุฑ ุงูุฅูุฌุงุจูุฉ/ุงูุณูุจูุฉ
- **ูุนุงูุฌุฉ ุฏูุนูุฉ**: ูุนุงูุฌุฉ ูุนุงูุฉ ููุตูุต ูุชุนุฏุฏุฉ

### โก ุงูุฃุฏุงุก
- **ูุธุงู ุชุฎุฒูู ูุคูุช**: ุนูููุงุช ูุญุณููุฉ ููุชูุฑุฑุฉ
- **ุฎูุงุฑุฒููุงุช ูุนุงูุฉ**: ุฃููุงุท regex ูุญุณููุฉ
- **ุชุญููู ูุณูู**: ููุงุกุฉ ูู ุงูุฐุงูุฑุฉ ูููุฌููุนุงุช ุงููุจูุฑุฉ

---

## ๐ฆ ุงูุชุซุจูุช

ุฃุถู ูุฐุง ุฅูู ููู `pubspec.yaml`:

```yaml
dependencies:
  tokenizer_moqasem: ^1.0.0
```

ุซู ููุฐ:

```bash
flutter pub get
```

---

## ๐ ุงูุงุณุชุฎุฏุงู

### ุงูุชูุทูุน ุงูุฃุณุงุณู

```dart
import 'package:tokenizer_moqasem/tokenizer_moqasem.dart';

// ุฅูุดุงุก tokenizer ุจุงูุฅุนุฏุงุฏุงุช ุงูุงูุชุฑุงุถูุฉ
final tokenizer = Tokenizer();

// ุชูุทูุน ุงููุต
final text = 'ูุฑุญุจุงู ุจู ูู ุนุงูู ุงูุจุฑูุฌุฉ';
final tokens = tokenizer.tokenize(text);
print(tokens); // [ูุฑุญุจุง, ุจู, ูู, ุนุงูู, ุงูุจุฑูุฌุฉ]

// ุนุฏ ุงููููุงุช
print(tokenizer.countTokens(text)); // 5

// ุงููููุงุช ุงููุฑูุฏุฉ
print(tokenizer.uniqueTokens(text));
```

### ุฅุนุฏุงุฏุงุช ูุฎุตุตุฉ

```dart
final config = TokenizerConfig(
  type: TokenizationType.word,
  toLowerCase: true,
  removePunctuation: true,
  removeStopWords: true,
  normalizeArabic: true,
  useFuzzyMatching: true,
  fuzzyThreshold: 0.7,
);

final tokenizer = Tokenizer(config: config);
```

### ุงููุทุงุจูุฉ ุงูุบุงูุถุฉ (ุชุญูู ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ)

```dart
final tokenizer = Tokenizer(
  config: TokenizerConfig(
    useFuzzyMatching: true,
    fuzzyThreshold: 0.7,
  ),
);

// ููุงุฑูุฉ ุฌูู ุชุญุชูู ุนูู ุฃุฎุทุงุก ุฅููุงุฆูุฉ
final query = 'ุงูุทุงูุจ ุฏูุจ ุงูู ุงููุฏุฑุณู'; // ูุญุชูู ุนูู ุฃุฎุทุงุก
final sentences = [
  'ุงูุทุงูุจ ุฐูุจ ุฅูู ุงููุฏุฑุณุฉ',
  'ุงูุชูููุฐ ุฐูุจ ูููุฏุฑุณุฉ',
  'ุงูุทุงูุจ ุฌุงุก ูู ุงููุฏุฑุณุฉ',
];

final results = tokenizer.compareSentences(query, sentences);
for (final result in results) {
  print('${result.rank}. ${result.percentage.toStringAsFixed(2)}% - ${result.sentence}');
}
```

### ูุนุงูุฌุฉ ุงููุตูุต ุงูุนุฑุจูุฉ

```dart
// ุชุทุจูุน ุงููุต ุงูุนุฑุจู
final text = 'ุฃููุณูููุงูู ุนููููููููู';
final normalized = tokenizer.normalizeArabicText(text);
print(normalized); // ุงูุณูุงู ุนูููู

// ุชุญููู ุงูุฃุฑูุงู ุงูุนุฑุจูุฉ
final arabicNum = 'ููกูขูฃูคูฅูฆูงูจูฉ';
final english = tokenizer.arabicToEnglishNumbers(arabicNum);
print(english); // 0123456789

// ุงุณุชุฎุฑุงุฌ ุงูุฃุฑูุงู
final mixedText = 'ูุฏู ูฅ ุชูุงุญุงุช ู 3 ุจุฑุชูุงูุงุช';
final numbers = tokenizer.extractNumbers(mixedText);
print(numbers); // {arabic: [ูฅ], english: [3]}
```

### ุญุณุงุจ ุงูุชุดุงุจู

```dart
// ุชุดุงุจู ุงููููุงุช
final sim = tokenizer.wordSimilarityPercentage('ูุงู', 'ุฏุงู');
print('${(sim * 100).toStringAsFixed(1)}%'); // 75.0%

// ุชุดุงุจู ุงูุฌูู (Cosine)
final sim1 = tokenizer.fuzzyCosineSimilarity(
  'ุฃุญุจ ุงูุจุฑูุฌุฉ',
  'ุฃุณุชูุชุน ุจุงูุจุฑูุฌุฉ'
);

// ุชุดุงุจู ุงูุฌูู (Jaccard)
final sim2 = tokenizer.fuzzyJaccardSimilarity(
  'ูุฑุญุจุงู ุจู',
  'ูุฑุญุจุง ุจูู'
);
```

### ูุดู ุงููุบุฉ

```dart
final lang1 = tokenizer.detectLanguage('Hello World');
print(lang1); // Language.english

final lang2 = tokenizer.detectLanguage('ูุฑุญุจุงู ุจู');
print(lang2); // Language.arabic

final lang3 = tokenizer.detectLanguage('Hello ูุฑุญุจุงู');
print(lang3); // Language.mixed
```

### N-Gram ุงูุชูุทูุน

```dart
final tokenizer = Tokenizer(
  config: TokenizerConfig(
    type: TokenizationType.ngram,
    ngramSize: 2,
  ),
);

final text = 'ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ';
final ngrams = tokenizer.tokenize(text);
print(ngrams); // [ูุนุงูุฌุฉ ุงููุบุฉ, ุงููุบุฉ ุงูุทุจูุนูุฉ]
```

### ุฅุญุตุงุฆูุงุช ุงููุต

```dart
final stats = tokenizer.getStatistics('ูุฑุญุจุงู ุจู ูู ุนุงูู ุงูุจุฑูุฌุฉ ุงูุฌููู');
print(stats);
// ุฅุฌูุงูู ุงููููุงุช: 6
// ุงููููุงุช ุงููุฑูุฏุฉ: 6
// ูุชูุณุท ุทูู ุงููููุฉ: 4.5
// ุงูุชููุน ุงููุบูู: 100%
// ูููุงุช ุนุฑุจูุฉ: 6
```

### ุงุณุชุฎุฑุงุฌ ุงูุฌุฐูุฑ (Stemming)

```dart
final stem = ArabicStemmer.stem('ุงููุฏุฑุณุฉ');
print(stem); // ูุฏุฑุณ

final root = ArabicStemmer.extractRoot('ูุงุชุจูู');
print(root); // ูุชุจ

// ููุงุฑูุฉ ุงูุฌุฐูุฑ
final same = ArabicStemmer.haveSameRoot('ูุชุจ', 'ูุงุชุจ');
print(same); // true
```

### ุงููุทุงุจูุฉ ุงูุตูุชูุฉ

```dart
// ุชุดุงุจู ุตูุชู ูููููุงุช ุงูุนุฑุจูุฉ
final sim = PhoneticMatcher.calculatePhoneticSimilarity('ูุงู', 'ุฏุงู');
print('ุงูุชุดุงุจู ุงูุตูุชู: ${(sim * 100).toStringAsFixed(1)}%');

// ุงูุจุญุซ ุนู ูููุงุช ูุชุดุงุจูุฉ ุตูุชูุงู
final matches = PhoneticMatcher.findPhoneticMatches(
  'ุตุงูุญ',
  ['ุณุงูู', 'ุตูุงุญ', 'ูุงูุญ', 'ุทุงูุญ'],
  threshold: 0.6,
);
for (final match in matches) {
  print(match); // ุตุงูุญ โ ุตูุงุญ (85% phonetic)
}
```

### ุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ

```dart
// ุชูุทูุน ูุตูุต ูุชุนุฏุฏุฉ
final texts = [
  'ุงููุต ุงูุฃูู',
  'ุงููุต ุงูุซุงูู',
  'ุงููุต ุงูุซุงูุซ',
];

final allTokens = tokenizer.tokenizeBatch(texts);

// ููุงุฑูุฉ ุฌููุน ุงูุฃุฒูุงุฌ (ูุตูููุฉ ุงูุชุดุงุจู)
final similarities = tokenizer.compareAllPairs(texts);
```

### ุฅุนุฏุงุฏุงุช ูุญุณููุฉ

```dart
// ูุญุณูู ููุนุฑุจูุฉ
final arabicTokenizer = Tokenizer(
  config: TokenizerConfig.arabicOptimized(),
);

// ูุญุณูู ููุฅูุฌููุฒูุฉ
final englishTokenizer = Tokenizer(
  config: TokenizerConfig.englishOptimized(),
);

// ูุทุงุจูุฉ ุตุงุฑูุฉ (ุจุฏูู fuzzy)
final strictTokenizer = Tokenizer(
  config: TokenizerConfig.strictMatching(),
);
```

---

## ๐จ ุฃูุซูุฉ ูุชูุฏูุฉ

### ุงูุจุญุซ ุนู ุฌูู ูุชุดุงุจูุฉ ูุน ุชูุงุตูู ุงููููุงุช

```dart
final tokenizer = Tokenizer(
  config: TokenizerConfig(useFuzzyMatching: true),
);

final query = 'ููุงู ูู ุงุฐูุจ';
final sentences = [
  'ูุงู ูู ุงุฐูุจ ูููุฏุฑุณุฉ',
  'ู ุฏุงู ูู ุงุฐูุจ',  // ุฎุทุฃ ุฅููุงุฆู
  'ููุงู ููู ุงุฐูุจูุง',
];

final results = tokenizer.compareSentences(
  query,
  sentences,
  showWordMatches: true, // ุฅุธูุงุฑ ุชุทุงุจู ุงููููุงุช
);

for (final result in results) {
  print(result);
  // ูุนุฑุถ:
  // - ุงูุชุฑุชูุจ
  // - ูุณุจุฉ ุงูุชุดุงุจู
  // - ุชุทุงุจู ูู ูููุฉ ุนูู ุญุฏุฉ
}
```

### ูููุงุช ุชููู ูุฎุตุตุฉ

```dart
final customStopWords = {'ูู', 'ูู', 'ุฅูู', 'the', 'a', 'an'};

final tokenizer = Tokenizer(
  config: TokenizerConfig(
    removeStopWords: true,
    stopWords: customStopWords,
  ),
);
```

### ููุท Regex ูุฎุตุต

```dart
// ุงุณุชุฎุฑุงุฌ ุงูุฅุดุงุฑุงุช (@mentions)
final tokenizer = Tokenizer(
  config: TokenizerConfig(
    type: TokenizationType.custom,
    customPattern: RegExp(r'@[\w\u0600-\u06FF]+'),
  ),
);

final text = 'ูุฑุญุจุงู @ุฃุญูุฏ ู @ูุญูุฏ';
final mentions = tokenizer.tokenize(text);
print(mentions); // [@ุฃุญูุฏ, @ูุญูุฏ]
```

### ุญูุธ ูุงุณุชูุฑุงุฏ ุงูุฅุนุฏุงุฏุงุช

```dart
// ุญูุธ ุงูุฅุนุฏุงุฏุงุช
final config = TokenizerConfig(
  type: TokenizationType.word,
  toLowerCase: true,
  fuzzyThreshold: 0.8,
);

final jsonString = config.toJsonString();
// ุญูุธ ูู ูุงุนุฏุฉ ุจูุงูุงุช ุฃู ููู

// ุงุณุชูุฑุงุฏ ุงูุฅุนุฏุงุฏุงุช
final loadedConfig = TokenizerConfig.fromJsonString(jsonString);
final tokenizer = Tokenizer(config: loadedConfig);
```

---

## ๐ก ูุตุงุฆุญ ููุฃุฏุงุก

1. **ุงุณุชุฎุฏู ุงูุชุฎุฒูู ุงููุคูุช**: ูุชู ุงูุชุฎุฒูู ุงููุคูุช ุชููุงุฆูุงู ููุนูููุงุช ุงููุชูุฑุฑุฉ
2. **ุงููุนุงูุฌุฉ ุงูุฏูุนูุฉ**: ุนุงูุฌ ูุตูุต ูุชุนุฏุฏุฉ ูุนุงู ููุฃุฏุงุก ุงูุฃูุถู
3. **ุถุจุท ุงูุนุชุจุฉ**: ุนุชุจุฉ ุฃูู (0.6-0.7) ูููุทุงุจูุฉ ุงูุฃุณุฑุน
4. **ุชุนุทูู ุงูููุฒุงุช**: ุฃููู ุงูููุฒุงุช ุบูุฑ ุงููุณุชุฎุฏูุฉ ูุซู ุญุฐู ูููุงุช ุงูุชููู

---

## ๐ค ุงููุณุงููุฉ

ุงููุณุงููุงุช ูุฑุญุจ ุจูุง! ูุง ุชุชุฑุฏุฏ ูู ุฅุฑุณุงู Pull Request.

## ๐ ุงูุชุฑุฎูุต

ูุฐุง ุงููุดุฑูุน ูุฑุฎุต ุจููุฌุจ MIT License - ุงูุธุฑ ููู [LICENSE](LICENSE) ููุชูุงุตูู.

## ๐ ุดูุฑ ูุชูุฏูุฑ

- ุชู ุงูุจูุงุก ุจูุณุจุฉ 95% ุจูุณุงุนุฏุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุจุงุณุชุฎุฏุงู Claude (Anthropic)
- ุดูุฑ ุฎุงุต ููุฌุชูุนุงุช Flutter ู Dart
- ูุนุงูุฌุฉ ุงููุบุฉ ุงูุนุฑุจูุฉ ูุณุชูุญุงุฉ ูู ุฃุจุญุงุซ ูุฎุชููุฉ ูู NLP

## ๐ง ุงูุชูุงุตู

- GitHub: [@Ahmed Ghaith](https://github.com/Ahmed0Ghaith)
- Email: devghaith@outlook.com

## ๐ ุงูุฏุนู

ุฅุฐุง ูุฌุฏุช ูุฐู ุงูุญุฒูุฉ ูููุฏุฉุ ูู ูุถูู ุฃุนุทูุง โญ๏ธ ุนูู [GitHub](https://github.com/Ahmed0Ghaith/tokenizer_moqasem)!

---

**ุตููุน ุจู โค๏ธ ู ๐ค AI** | ยฉ 2025 Ahmed Ghaith

---

## ๐ ุฃูุซูุฉ ุฅุถุงููุฉ

### ูุซุงู ูุงูู: ุชุทุจูู ุจุญุซ ุฐูู

```dart
class SmartSearch {
  final tokenizer = Tokenizer(
    config: TokenizerConfig(
      useFuzzyMatching: true,
      fuzzyThreshold: 0.7,
      normalizeArabic: true,
    ),
  );

  List<String> database = [
    'ูุชุงุจ ุงูุจุฑูุฌุฉ ุจูุบุฉ Dart',
    'ุชุนูู Flutter ูููุจุชุฏุฆูู',
    'ุฏููู ุชุทููุฑ ุชุทุจููุงุช ุงููุงุชู',
    'ุจุฑูุฌุฉ ุชุทุจููุงุช ุงูููุจ',
  ];

  List<SimilarityResult> search(String query) {
    return tokenizer.compareSentences(query, database);
  }
}

// ุงูุงุณุชุฎุฏุงู
void main() {
  final search = SmartSearch();
  final results = search.search('ูุชุจ ุจุฑูุฌู ููุงุชุฑ'); // ูุน ุฃุฎุทุงุก ุฅููุงุฆูุฉ
  
  for (final result in results) {
    if (result.percentage > 50) {
      print('โ ${result.sentence} - ${result.percentage.toStringAsFixed(1)}%');
    }
  }
}
```

### ูุซุงู: ุชุญููู ุชุบุฑูุฏุงุช

```dart
class TweetAnalyzer {
  final tokenizer = Tokenizer();

  Map<String, dynamic> analyze(String tweet) {
    return {
      'tokens': tokenizer.countTokens(tweet),
      'language': tokenizer.detectLanguage(tweet).name,
      'hashtags': _extractHashtags(tweet),
      'mentions': _extractMentions(tweet),
    };
  }

  List<String> _extractHashtags(String text) {
    final pattern = RegExp(r'#[\w\u0600-\u06FF]+');
    return pattern.allMatches(text).map((m) => m.group(0)!).toList();
  }

  List<String> _extractMentions(String text) {
    final pattern = RegExp(r'@[\w\u0600-\u06FF]+');
    return pattern.allMatches(text).map((m) => m.group(0)!).toList();
  }
}
